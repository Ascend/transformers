# Optimum Ascend
## ç®€ä»‹
æœ¬é¡¹ç›®å¼€å‘äº†Optimum Ascendæ’ä»¶ï¼Œç”¨äºæ˜‡è…¾NPUé€‚é…Transformerså¥—ä»¶ï¼Œä¸ºä½¿ç”¨Transformerså¥—ä»¶çš„å¼€å‘è€…æä¾›æ˜‡è…¾AIå¤„ç†å™¨çš„è¶…å¼ºç®—åŠ›ã€‚å½“å‰å·²ç»éªŒè¯çš„æ¨¡å‹å’Œä»»åŠ¡å¯ä»¥ä¸‹åœ¨[è¿™é‡Œ](https://gitee.com/ascend/transformers/tree/optimum#supported-models)æŸ¥è¯¢ã€‚ç”¨æˆ·ä¹Ÿå¯ä»¥è‡ªè¡Œå°è¯•è®­ç»ƒå…¶ä»–ä»»åŠ¡æˆ–æ¨¡å‹ã€‚


## å®‰è£…

```text
git clone https://gitee.com/ascend/transformers.git -b optimum && cd transformers
pip install -e .
```
> **è¯´æ˜**ï¼šå½“å‰é…å¥—ç‰ˆæœ¬å¦‚ä¸‹ï¼š
> - [Transformersv4.25.1](https://github.com/huggingface/transformers/tree/v4.25.1)
> - Torch + torch_npu 1.8.1 + apex

## ä½¿ç”¨è¯´æ˜
### NPUTrainerå’ŒNPUTrainingArgumentsç±»
è¿™é‡Œæœ‰ä¸¤ä¸ªä¸»è¦çš„ç±»éœ€è¦é‡ç‚¹è¯´æ˜ï¼š
- [NPUTrainer](https://gitee.com/ascend/transformers/blob/optimum/optimum/ascend/trainer.py): trainerç±»ç”¨äºåœ¨NPUä¸Šåˆ†å¸ƒå¼è®­ç»ƒå¹¶è¯„ä¼°æ¨¡å‹ã€‚
- [NPUTrainingArguments](https://gitee.com/ascend/transformers/blob/optimum/optimum/ascend/training_args.py): è¿™ä¸ªç±»ç”¨äºé…ç½®æ˜¯å¦å¼€å¯æ··åˆç²¾åº¦è®­ç»ƒä»¥åŠå…·ä½“è®¾ç½®ã€‚

[NPUTrainer](https://gitee.com/ascend/transformers/blob/optimum/optimum/ascend/trainer.py) å’ŒåŸç”Ÿå¥—ä»¶çš„ ğŸ¤— [Transformers Trainer](https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py)éå¸¸ç›¸ä¼¼ï¼Œå› æ­¤å¤§éƒ¨åˆ†ä½¿ç”¨`Trainer`å’Œ`TrainingArguments`çš„è®­ç»ƒè„šæœ¬åªéœ€è¦ç”¨`NPUTrainer`å’Œ`NPUTrainingArguments`æ›¿æ¢å°±èƒ½å·¥ä½œã€‚

åŸå§‹è„šæœ¬ï¼š
```python
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
  # training arguments...
)

# A lot of code here

# Initialize our Trainer
trainer = Trainer(
    model=model,
    args=training_args,  # Original training arguments.
    train_dataset=train_dataset if training_args.do_train else None,
    eval_dataset=eval_dataset if training_args.do_eval else None,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
    data_collator=data_collator,
)
```
å¯ä»¥åœ¨NPUä¸Šè¿è¡Œçš„è¿ç§»ç‰ˆæœ¬ï¼š
```diff
- from transformers import Trainer, TrainingArguments
+ from optimum.ascend import NPUTrainer, NPUTrainingArguments

- training_args = TrainingArguments(
+ training_args = NPUTrainingArguments(
  # training arguments...
)

# A lot of code here

# Initialize our Trainer
- trainer = Trainer(
+ trainer = NPUTrainer(
    model=model,
    args=training_args,  # Original training arguments.
    train_dataset=train_dataset if training_args.do_train else None,
    eval_dataset=eval_dataset if training_args.do_eval else None,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
    data_collator=data_collator,
)
```

å¦ä¸€ç§ç®€å•çš„æ–¹å¼æ˜¯ä½¿ç”¨ä¸€é”®è¿ç§»è„šæœ¬`transfor_to_npu`è‡ªåŠ¨æ›¿æ¢åŸå§‹è„šæœ¬ï¼š
```diff
+ from optimum.ascend import transfor_to_npu
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
  # training arguments...
)

# A lot of code here

# Initialize our Trainer
trainer = Trainer(
    model=model,
    args=training_args,  # Original training arguments.
    train_dataset=train_dataset if training_args.do_train else None,
    eval_dataset=eval_dataset if training_args.do_eval else None,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
    data_collator=data_collator,
)
```

### æ”¯æŒçš„åŠŸèƒ½
å½“å‰æ”¯æŒåœ¨æ˜‡è…¾NPUä¸Šå¯¹Transformerså¥—ä»¶è¿›è¡Œå•æœºå•å¡ã€å•æœºå¤šå¡è®­ç»ƒä»¥åŠï¼ˆç”±apexæ’ä»¶æä¾›çš„ï¼‰æ··åˆç²¾åº¦è®­ç»ƒã€‚è¿™é‡Œä»¥[é—®ç­”ä»»åŠ¡](https://gitee.com/ji-huazhong/transformers_test/tree/master/examples/question-answering)ä¸ºä¾‹è¯´æ˜æ’ä»¶çš„ç”¨æ³•ã€‚
å•å¡è®­ç»ƒè„šæœ¬å¦‚ä¸‹ï¼š
```bash
python3.7 run_qa.py \
        --model_name_or_path albert-base-v2 \
        --dataset_name squad \
        --do_train \
        --do_eval \
        --num_train_epochs 3 \
        --learning_rate 3e-5 \
        --device_id 0 \
        --half_precision_backend apex \
        --fp16 \
        --fp16_opt_level O2 \
        --optim adamw_apex_fused_npu \
        --use_combine_grad \
        --loss_scale 1024.0 \
        --dataloader_drop_last \
        --output_dir ./output
```
è®­ç»ƒè„šæœ¬å‚æ•°è¯´æ˜å¦‚ä¸‹ï¼š
```text
--output_dir             // è®­ç»ƒç»“æœå’Œcheckpointä¿å­˜è·¯å¾„
--num_train_epochs       // è®­ç»ƒçš„epochæ¬¡æ•°
--model_name_or_path     // ä¸è®­ç»ƒæ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„
--dataset_name           // æ•°æ®é›†åç§°
--do_train               // æ‰§è¡Œè®­ç»ƒ
--do_eval                // æ‰§è¡Œè¯„ä¼°
--dataloader_drop_last   // ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„batch
--learning_rate          // åˆå§‹å­¦ä¹ ç‡
--device_id              // [æ’ä»¶æ–°å¢å‚æ•°]æŒ‡å®šå•å¡è®­ç»ƒæ—¶çš„å¡å·ï¼Œå¤šå¡è®­ç»ƒæ— éœ€ä½¿ç”¨
--half_precision_backend // æŒ‡å®šæ··åˆç²¾åº¦ä½¿ç”¨çš„åç«¯ï¼Œå½“å‰åªæ”¯æŒapexï¼Œéœ€è¦æ‰‹åŠ¨æŒ‡å®š
--fp16                   // ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œå½“å‰åªæ”¯æŒapex
--fp16_opt_level         // apexæ··åˆç²¾åº¦çº§åˆ«
--optim                  // æŒ‡å®šä½¿ç”¨çš„ä¼˜åŒ–å™¨ï¼Œå½“å‰å¯ä»¥ä½¿ç”¨NPUäº²å’Œçš„èåˆä¼˜åŒ–å™¨ adamw_apex_fused_npu
--loss_scale             // [æ’ä»¶æ–°å¢å‚æ•°]apexæ··åˆç²¾åº¦è®­ç»ƒä½¿ç”¨å›ºå®šloss_scaleï¼Œç”¨äºæ€§èƒ½è°ƒä¼˜
--use_combine_grad       // [æ’ä»¶æ–°å¢å‚æ•°]apexæ··åˆç²¾åº¦è®­ç»ƒä½¿ç”¨combine_gradé€‰é¡¹ï¼Œç”¨äºæ€§èƒ½è°ƒä¼˜
```

## éªŒè¯çš„æ¨¡å‹
Optimum Ascendæ’ä»¶æ”¯æŒçš„æ¨¡å‹å’Œä¸‹æ¸¸ä»»åŠ¡è§ä¸‹è¡¨ã€‚

| Architecture | Single Card | Multi Card | Tasks                                                                 |
|--------------|-------------|------------|-----------------------------------------------------------------------|
| BERT         | âœ”ï¸          | âœ”ï¸         | <li>[text classification](https://gitee.com/ji-huazhong/transformers_test/blob/master/examples/text-classification/test/train_bert_base_cased_full_8p.sh)</li><br><li>[token classification](https://gitee.com/ji-huazhong/transformers_test/blob/master/examples/token-classification/test/train_bert_base_NER_full_8p.sh)</li> |
| ALBERT       | âœ”ï¸          | âœ”ï¸         | <li>[question answering](https://gitee.com/ji-huazhong/transformers_test/blob/master/examples/question-answering/test/train_albert_base_v2_full_8p.sh)</li>                                       |                     |
| RoBERTa      | âœ”ï¸          | âœ”ï¸         | <li>[language modeling](https://gitee.com/ji-huazhong/transformers_test/blob/master/examples/language-modeling/test/train_roberta_base_full_8p.sh)</li>                                        |
| T5           | âœ”ï¸          | âœ”ï¸         | <li>[translation](https://gitee.com/ji-huazhong/transformers_test/blob/master/examples/translation/test/train_t5_small_full_8p.sh)</li>                                              |
| GPT-2        | âœ”ï¸          | âœ”ï¸         | <li>[language modeling](https://gitee.com/ji-huazhong/transformers_test/blob/master/examples/language-modeling/test/train_gpt2_full_8p.sh)</li>                                        |
> Transformerså¥—ä»¶æ”¯æŒçš„å…¶ä»–æ¨¡å‹æˆ–ä¸‹æ¸¸ä»»åŠ¡å¯èƒ½ä¹Ÿèƒ½è¢«Optimum Ascendæ’ä»¶æ”¯æŒï¼Œç”¨æˆ·å¯ä»¥è‡ªè¡Œå°è¯•ã€‚

## é…ç½®NPUè®­ç»ƒç¯å¢ƒ
è¯·å‚è€ƒæ˜‡è…¾å®˜æ–¹çš„ã€Š[Pytorchæ¡†æ¶è®­ç»ƒç¯å¢ƒå‡†å¤‡](https://www.hiascend.com/document/detail/zh/ModelZoo/pytorchframework/ptes)ã€‹ã€‚

## ç‰ˆæœ¬è¯´æ˜
### å˜æ›´
2023.03.29ï¼šé¦–æ¬¡å‘å¸ƒ

2023.04.04ï¼štranslationä¸‹æ¸¸ä»»åŠ¡evalé˜¶æ®µä½¿ç”¨äºŒè¿›åˆ¶ä»¥è§£å†³ç®—å­ç¼–è¯‘è¿‡é•¿å¯¼è‡´çš„æ‰å¡é—®é¢˜

## FQA